{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, neurons, activation, input_shape = None):\n",
    "        self.activation = activation\n",
    "        self.bias = np.random.rand(neurons, 1)\n",
    "        self.neurons = neurons\n",
    "        self.initialiseWeights(input_shape)\n",
    "    \n",
    "    def initialiseWeights(self, input_shape):\n",
    "        if input_shape != None:\n",
    "            self.weights = np.random.rand(self.neurons, input_shape[1] * input_shape[2])\n",
    "        else:\n",
    "            self.weights = []\n",
    "            \n",
    "    # activation functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        for i in range(z.shape[0]):\n",
    "            if z[i] < 0:\n",
    "                z[i] = 0\n",
    "        return z\n",
    "    \n",
    "    def leaky_relu(self, z):\n",
    "        if z > 0:\n",
    "            return z\n",
    "        else:\n",
    "            return 0.01 * z\n",
    "    \n",
    "    def softmax(self, outputs):\n",
    "        e = np.exp(outputs)\n",
    "        return e / e.sum()\n",
    "    \n",
    "    # activation function gradients\n",
    "    def relu_gradient(self, result):\n",
    "        dev = np.array([])\n",
    "        for x in result:\n",
    "            if x > 0:\n",
    "                dev = np.append(dev,1)\n",
    "            else:\n",
    "                dev = np.append(dev, 0)\n",
    "        return dev\n",
    "    \n",
    "    def sigmoid_gradient(self, result):\n",
    "        return sigmoid(result) * (1-sigmoid(result))\n",
    "    \n",
    "    # forward and backward pass\n",
    "    def forwardPass(self, input_data):\n",
    "        self.input = input_data\n",
    "        if len(self.weights) == 0:\n",
    "            input_data = input_data.reshape(1, input_data.shape[0], input_data.shape[1])\n",
    "            self.initialiseWeights(input_data.shape)\n",
    "        self.output = np.dot(self.weights, self.input) + self.bias\n",
    "        \n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.output = self.sigmoid(self.output)\n",
    "        elif self.activation == \"relu\":\n",
    "            self.output = self.relu(self.output)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backwardPass(self, mask, learning_rate):\n",
    "        input_error = np.dot(self.weights.T, mask)\n",
    "        weights_error = np.dot(self.input, mask.T)\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error.T\n",
    "        self.bias -= learning_rate * mask\n",
    "        \n",
    "        return input_error\n",
    "    \n",
    "    # Optimizers\n",
    "    def gradientDescent(self, alpha, gradient):\n",
    "        self.weights = self.weights - alpha * gradient\n",
    "\n",
    "    def adaGrad(self, alpha, gradient, Sum):\n",
    "        Sum += gradient ** 2\n",
    "        alpha = alpha / np.sqrt(Sum)\n",
    "        self.weights = self.weights - alpha * gradient\n",
    "\n",
    "    def RMSProp(self, alpha, gradient, average, beta=0.9):\n",
    "        average = beta * average + (1 - beta) * gradient * gradient\n",
    "        alpha = alpha / np.sqrt(average)\n",
    "        self.weights = self.weights - alpha * gradient\n",
    "\n",
    "    def adam(self, c, a, alpha, gradient, beta1=0.9, beta2=0.9):\n",
    "        c = beta1 * c + (1 - beta1) * gradient\n",
    "        a = beta2 * a + (1 - beta2) * np.power(gradient, 2)\n",
    "        alpha = alpha / np.sqrt(a)\n",
    "        self.weights = self.weights - alpha * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def fit(self, train_X, train_y, epochs, learning_rate, loss):\n",
    "        samples = len(train_X)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            \n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = train_X[j]\n",
    "                output = output.reshape(output.shape[0] * output.shape[1], 1)\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forwardPass(output)\n",
    "\n",
    "                # compute loss\n",
    "                if loss == \"hinge-loss\":\n",
    "                    err_, mask  = self.hinge_loss(output, train_y[j], 1)\n",
    "                elif loss == \"cross-entropy-loss\":\n",
    "                    err_, mask = self.cross_entropy_loss(output, train_y[j])\n",
    "\n",
    "                error = mask\n",
    "                # backward propagation\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backwardPass(error, learning_rate)\n",
    "            \n",
    "    def predict(self, test_X, test_y):\n",
    "        predictions = []\n",
    "        for test_point in test_X:\n",
    "            output = test_point\n",
    "            output = output.reshape(output.shape[0] * output.shape[1], 1)\n",
    "            for layer in self.layers:\n",
    "                output = layer.forwardPass(output)\n",
    "            y = output.argmax(axis=-1)\n",
    "            predictions.append(y)\n",
    "            \n",
    "#         print(predictions)\n",
    "#         print(test_y)\n",
    "        \n",
    "    # loss functions\n",
    "    def cross_entropy_loss(self, scores, true_class):\n",
    "        scores_temp = np.copy(scores)\n",
    "        scores_temp -= scores.max(keepdims = True)\n",
    "        probs = np.exp(scores)/np.sum(np.exp(scores), keepdims = True)\n",
    "        loss = -np.log(probs[true_class])\n",
    "        loss = np.sum(loss)\n",
    "        \n",
    "        dscores[true_class] -= 1\n",
    "        \n",
    "        return loss, dscores\n",
    "    \n",
    "\n",
    "    def hinge_loss(self, scores, true_class, theta):\n",
    "        correct_class_score = scores[true_class]\n",
    "        margin = np.maximum(0, scores - correct_class_score + theta)\n",
    "        margin[true_class] = 0\n",
    "        \n",
    "        valid_margin_mask = np.zeros(margin.shape)\n",
    "        valid_margin_mask[margin > 0] = 1 \n",
    "        valid_margin_mask[true_class] = -np.sum(valid_margin_mask)\n",
    "        \n",
    "        return np.sum(margin), valid_margin_mask\n",
    "    \n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean(np.power(y_true-y_pred, 2));\n",
    "    \n",
    "    def mse_gradient(self, y_true, y_pred):\n",
    "        return 2*(y_pred-y_true)/y_true.size;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
