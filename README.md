<h1 align="center">Neural Network Implementation</h1>

### Description
`Neural Network` implemented with different `Activation Functions`, `Optimizers`, and `Loss Functions`. 

### Activation Functions
- Sigmoid
- Relu
- Leaky-Relu
- Softmax

### Optimizers
- Gradient Descent
- AdaGrad
- RMSProp
- Adam

### Loss Functions
- Cross-Entropy Loss
- Hinge-Loss
- Mean Squared Error (MSE)

### Contributors
- [Sameet Asadullah](https://github.com/SameetAsadullah)
- [Aysha Noor](https://github.com/ayshanoorr)
